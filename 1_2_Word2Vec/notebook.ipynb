{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "arbitrary-vertex",
   "metadata": {},
   "source": [
    "# Word2Vec - Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-execution",
   "metadata": {},
   "source": [
    "## 0. import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "working-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civic-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-custody",
   "metadata": {},
   "source": [
    "## 1. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-convertible",
   "metadata": {},
   "source": [
    "### text preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "arbitrary-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    data_path: str,\n",
    "    word_index: dict = None,\n",
    "    num_words: int = 10000,\n",
    "):\n",
    "    tokenizer = Mecab()\n",
    "\n",
    "    # 0. data load\n",
    "    with open(data_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # 1. bag-of-words\n",
    "    vocab, docs = [], []\n",
    "    for doc in tqdm(data):\n",
    "        if doc:\n",
    "            # nsmc 데이터에 nan값을 제외해주기 위함\n",
    "            try:\n",
    "                nouns = tokenizer.nouns(doc)\n",
    "                vocab.extend(nouns)\n",
    "                docs.append(nouns)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # 2. build vocab\n",
    "    if not word_index:\n",
    "        vocab = Counter(vocab)\n",
    "        vocab = vocab.most_common(num_words)\n",
    "\n",
    "        # 3. add unknwon token\n",
    "        word_index = {\"<UNK>\": 0}\n",
    "        for idx, (word, _) in enumerate(vocab, 1):\n",
    "            word_index[word] = idx\n",
    "\n",
    "    index_word = {idx: word for word, idx in word_index.items()}\n",
    "\n",
    "    # 4. create corpus\n",
    "    corpus = []\n",
    "    for doc in docs:\n",
    "        if doc:\n",
    "            corpus.append([word_index.get(word, 0) for word in doc])\n",
    "\n",
    "    return corpus, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forbidden-marathon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0063d0d1bf4a7bab442447f2e3f361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952a87e0c78a4e49be13b849eb0dbc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_path = \"../data/nsmc/train_data.pkl\"\n",
    "test_path = \"../data/nsmc/test_data.pkl\"\n",
    "\n",
    "train_corpus, word_index, index_word = preprocess(train_path)\n",
    "test_corpus, _, _ = preprocess(test_path, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-yeast",
   "metadata": {},
   "source": [
    "### skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "revolutionary-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py\n",
    "def skipgrams(\n",
    "    sequence: List[int],\n",
    "    vocab_size: int,\n",
    "    window_size: int = 4,\n",
    "    negative_samples: int = 1.0,\n",
    "):\n",
    "    couples, labels = [], []\n",
    "    for i, wi in enumerate(sequence):\n",
    "        if not wi:  # <UNK> 토큰일 경우\n",
    "            continue\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(sequence), i + window_size + 1)\n",
    "        #     print(window_start, window_end)\n",
    "\n",
    "        for j in range(window_start, window_end):\n",
    "            if j != i:\n",
    "                wj = sequence[j]\n",
    "                if not wj:  # <UNK> 토큰일 경우\n",
    "                    continue\n",
    "\n",
    "                couples.append([wi, wj])\n",
    "                labels.append(1)\n",
    "\n",
    "    if negative_samples > 0:\n",
    "        num_negative_samples = int(len(labels) * negative_samples)\n",
    "        words = [c[0] for c in couples]\n",
    "        random.shuffle(words)\n",
    "\n",
    "        couples += [\n",
    "            [words[idx % len(words)], random.randint(1, vocab_size - 1)]\n",
    "            for idx in range(num_negative_samples)\n",
    "        ]\n",
    "\n",
    "        labels += [0] * num_negative_samples\n",
    "\n",
    "    return couples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "refined-consumer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8068217de67246f0ba85c423b08bb2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/141731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train skipgrams\n",
    "vocab_size = len(word_index)\n",
    "train_pairs, train_labels = [], []\n",
    "for sequence in tqdm(train_corpus):\n",
    "    pairs, targets = skipgrams(sequence, vocab_size, negative_samples=0.5)\n",
    "    train_pairs.extend(pairs)\n",
    "    train_labels.extend(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "impaired-principal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c866e3b722ae42e9b7a057d27d52ba14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test skipgrams\n",
    "vocab_size = len(word_index)\n",
    "test_pairs, test_labels = [], []\n",
    "for sequence in tqdm(test_corpus):\n",
    "    pairs, targets = skipgrams(sequence, vocab_size, negative_samples=0.5)\n",
    "    test_pairs.extend(pairs)\n",
    "    test_labels.extend(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-shuttle",
   "metadata": {},
   "source": [
    "### DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "public-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VDataset(Dataset):\n",
    "    def __init__(self, pairs: List[List[int]], labels: List[int]):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pending-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = W2VDataset(train_pairs, train_labels)\n",
    "testset = W2VDataset(test_pairs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "attached-grace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[77, 12], [77, 319]], [1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-cheese",
   "metadata": {},
   "source": [
    "### collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "polyphonic-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    targets = [entry[0][0] for entry in batch]\n",
    "    contexts = [entry[0][1] for entry in batch]\n",
    "    labels = [entry[1] for entry in batch]\n",
    "\n",
    "    targets = torch.LongTensor(targets)\n",
    "    contexts = torch.LongTensor(contexts)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-brother",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cardiac-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=trainset,\n",
    "    batch_size=256,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=testset,\n",
    "    batch_size=256,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pleasant-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    sample = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pregnant-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-image",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-import",
   "metadata": {},
   "source": [
    "## 2. Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "chinese-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(pl.LightningModule):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 100):\n",
    "        super(Word2Vec, self).__init__()\n",
    "\n",
    "        self.input_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.output_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        u = self.input_embed(target)  # [batch_size, embed_dim]\n",
    "        v = self.output_embed(context)  # [batch_size, embed_dim]\n",
    "\n",
    "        score = torch.sum(u * v, dim=1)  # [batch_size]\n",
    "        return score\n",
    "\n",
    "    def loss_fn(self, logits, labels):\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        loss = criterion(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def accuracy(self, logits, labels):\n",
    "        logits = torch.round(torch.sigmoid(logits))\n",
    "        corrects = (logits == labels).float().sum()\n",
    "        acc = corrects / labels.numel()\n",
    "        return acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        targets, contexts, labels = train_batch\n",
    "        logits = self.forward(targets, contexts)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        acc = self.accuracy(logits, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        targets, contexts, labels = val_batch\n",
    "        logits = self.forward(targets, contexts)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        acc = self.accuracy(logits, labels)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-harvest",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-worth",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faced-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "model = Word2Vec(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "biblical-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=2,\n",
    "    max_epochs=10,\n",
    "    val_check_interval=0.5,\n",
    "    accelerator=\"dp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "assured-renewal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type      | Params\n",
      "-------------------------------------------\n",
      "0 | input_embed  | Embedding | 1.0 M \n",
      "1 | output_embed | Embedding | 1.0 M \n",
      "-------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f287d7bc694a7cb11debec8f3ff8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/excelsiorcjh/miniconda3/envs/pt-py37/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/excelsiorcjh/miniconda3/envs/pt-py37/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-improvement",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-inspection",
   "metadata": {},
   "source": [
    "## 4. Check using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "instant-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-roberts",
   "metadata": {},
   "source": [
    "### create pre-trained vectors file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cleared-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.input_embed.weight\n",
    "embedding = embedding.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "direct-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100\n",
    "with open(\"./vectors.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(f\"{len(word_index)-1} {embed_dim}\\n\")\n",
    "    for word, idx in word_index.items():\n",
    "        if idx != 0:\n",
    "            str_vec = \" \".join(map(str, list(embedding[idx, :])))\n",
    "            f.write(f\"{word} {str_vec}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "postal-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(\"./vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "compressed-subscription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영화', 0.8665787577629089),\n",
       " ('연기', 0.8557248115539551),\n",
       " ('스토리', 0.8543652296066284),\n",
       " ('최고', 0.8537499308586121),\n",
       " ('나', 0.8484581112861633),\n",
       " ('내용', 0.8468432426452637),\n",
       " ('생각', 0.8454784154891968),\n",
       " ('것', 0.8232203722000122),\n",
       " ('수', 0.8192157745361328),\n",
       " ('듯', 0.8190522193908691)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(\"배우\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-chicago",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
